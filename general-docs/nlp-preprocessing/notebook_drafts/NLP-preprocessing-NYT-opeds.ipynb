{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for preprocessing NYT op-ed data\n",
    "\n",
    "Goal: Emily & Greg go through NLP preprocessing pipeline for two data sets in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1. Read in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names = range(1,14)\n",
    "df_list = []\n",
    "\n",
    "for name in names:\n",
    "    csvfile = '/Users/emilyhalket/Desktop/NLP_NYT/datafiles/{0}_100.csv'.format(name)\n",
    "    df = pd.read_csv(csvfile)\n",
    "    df_list.append(df)\n",
    "\n",
    "article_df = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "article_df = article_df[pd.notnull(article_df['full_text'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11570, 11)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset has __11,648__ op-eds from the NY Times. We have additional information for each article (title, author, number of comments, etc.) but for now we will just focus on the text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenize\n",
    "\n",
    "For my analysis, I plan to consider each article as a separate document. For my purposes, I do not need to retain punctuation information, so I plan to remove punctuation in my preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_article_content(text_df):\n",
    "\n",
    "    print 'preprocessing article text...'\n",
    "\n",
    "    # text_df is data frame from SQL query, column 'content' contains text content from each article\n",
    "    article_list = []\n",
    "\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    stop_words = set(stopwords.words('english'))  # can add more stop words to this set\n",
    "    \n",
    "    stemmer = SnowballStemmer('english')\n",
    "\n",
    "    kept_rows = [] # keep track of rows that have unusable articles\n",
    "\n",
    "    for row, article in enumerate(text_df['full_text']):\n",
    "\n",
    "        cleaned_tokens = []\n",
    "\n",
    "        tokens = tokenizer.tokenize(article.decode('utf-8').lower())\n",
    "\n",
    "        for token in tokens:\n",
    "\n",
    "            \n",
    "            if token not in stop_words:\n",
    "\n",
    "                if len(token) > 0 and len(token) < 20: # removes non words\n",
    "\n",
    "                    if not token[0].isdigit() and not token[-1].isdigit(): # removes numbers\n",
    "                        \n",
    "                        stemmed_tokens = stemmer.stem(token)\n",
    "                        cleaned_tokens.append(stemmed_tokens)\n",
    "\n",
    "        print 'success for row %d' % row \n",
    "        article_list.append(' '.join(wd for wd in cleaned_tokens))\n",
    "        kept_rows.append(row)\n",
    "\n",
    "    print 'preprocessed content for %d articles' % len(article_list)\n",
    "\n",
    "    return article_list, kept_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "article_df = article_df[pd.notnull(article_df['full_text'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11570, 11)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing article text...\n",
      "preprocessed content for 11570 articles\n"
     ]
    }
   ],
   "source": [
    "article_list, kept_rows = preprocess_article_content(article_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11570"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(article_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'long deni referendum independ akin held scotland quebec catalan separatist proclaim sunday elect region parliament would de facto plebiscit secess spain result howev muddl issu separatist parti win major seat fail win major vote would requir referendum amount mandat creat new nation still vote tell conserv govern prime minist mariano rajoy better start heed catalan like scottish quebec separatist catalan separatist convinc prosper region would better far certain especi sinc membership european union alreadi face major challeng uniti hard guarante also certainti european central bank would continu fund catalan bank hit euro debt crisi given clear choic whether go alon scot quebec pull back scot year ago quebec catalan howev abl make choic spain constitut enshrin indissolubl uniti mr rajoy use block discuss self determin catalonia deni referendum separatist leader artur mas pledg start process toward independ parti won elect togeth yes coalit fail win major seat join forc far left separatist parti oppos mr mas leadership form major region parliament even togeth separatist parti poll percent vote result empow separatist unilater break spain deni catalan choic matter certain deepen nationalist feel best cours separatist use strength seek control affair madrid mr rajoy recogn even within constraint spanish constitut plenti room discuss accommod catalan yearn'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_list[2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
