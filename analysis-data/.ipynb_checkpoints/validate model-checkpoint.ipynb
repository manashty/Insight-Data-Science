{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation of models\n",
    "\n",
    "\n",
    "Our models find similarities among articles, but how can we test that the similarities make sense?   \n",
    "\n",
    "In the \"AutismParentMagazine\" website each article is assigned to a **category**, and we can use this classification to validate the model.\n",
    "\n",
    "\n",
    "Let's define a function to tell wether two articles ($1$ and $2$) belong to the same category:     \n",
    "\n",
    "\\begin{eqnarray}\n",
    "f (1,2)  &=& 1 & \\mbox{ if } 1\\mbox{, } 2 \\mbox{ in same category, or}\\nonumber\\\\   \n",
    "         &=& 0 & \\mbox{ elsewhere.}\\nonumber\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "If the model finds that two articles ($i$, $j$) are similar and $f(i,j)=1$ (they belong to the same category), then these are a good match; viceversa if $f(i,j)=0$ they may not be good match.\n",
    "\n",
    "We can then evaluate the model, for all $N$ pairs of similar articles $i$, $j$, as:   \n",
    "\\begin{eqnarray}\n",
    "\\mbox{score} = \\frac{1}{N} \\sum_{i,j} f(i,j) .\n",
    "\\end{eqnarray}\n",
    "\n",
    "To illustrate, if the model finds that all articles pairs found to be similar also belong to the same category, then the score is 1.   \n",
    "In fact, the closer the score is to 1, the better the model is.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from gensim import corpora, models, similarities\n",
    "\n",
    "\n",
    "def get_model_score(ids,matsim,categories):\n",
    "    \"\"\" Function to evalate the score for a given model, following the equation defined above\"\"\"\n",
    "    num_of_predictions=3\n",
    "    model_score=0\n",
    "    for id,doc in zip(ids,matsim.index):\n",
    "        sims=matsim[doc]\n",
    "        for other_id,score in sims:\n",
    "            #print(\"ID {} OTHER_ID {} SCORE {}\".format(id,other_id,score))\n",
    "            category1=categories[id]\n",
    "            category2=categories[other_id]\n",
    "            if id != other_id:\n",
    "                if category1 == category2:\n",
    "                    model_score+=1\n",
    "    N=len(ids)*num_of_predictions\n",
    "    model_score=model_score/N\n",
    "    return model_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read datasets from file:\n",
    "\n",
    "os.chdir('../data/')\n",
    "# Read dataframe\n",
    "input_fname=\"AutismParentMagazine-posts-tokens.csv\"\n",
    "\n",
    "# Get categories and ids from dataset\n",
    "df = pd.read_csv(input_fname,index_col=0)\n",
    "df.head(2)\n",
    "categories=df['category']\n",
    "ids=df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSI model score 0.6851851851851852\n",
      "LDA model score 0.24074074074074073\n"
     ]
    }
   ],
   "source": [
    "# Read models and evaluate the score\n",
    "import pickle\n",
    "\n",
    "# Read models\n",
    "matsim = pickle.load(open(\"lsi-matsim.save\", \"rb\"))\n",
    "model_score= get_model_score(ids,matsim,categories)\n",
    "print(\"LSI model score {}\".format(model_score))\n",
    "\n",
    "# Read models\n",
    "matsim = pickle.load(open(\"lda-matsim.save\", \"rb\"))\n",
    "model_score= get_model_score(ids,matsim,categories)\n",
    "print(\"LDA model score {}\".format(model_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take home message:    \n",
    "\n",
    "The LSI model seems to perform better in this case.    \n",
    "A score of 0.68 can be interpreted as ~70% of similar articles also belong to the same category.\n",
    "\n",
    "On the other side, the LDA model has a much lower score 0.24, and hence is not performing well. Probably the performance could be improved by twicking the model parameters (number of topics, etc.).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
